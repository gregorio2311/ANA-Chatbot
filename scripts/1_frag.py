"""
PROCESADOR DE FRAGMENTOS - ANA-CHATBOT
=====================================

Este script procesa documentos Word (.docx) de anatom√≠a y los divide en fragmentos
de texto optimizados para b√∫squeda sem√°ntica. Es el primer paso del pipeline.

FUNCIONALIDADES:
- Extrae texto de documentos Word (.docx)
- Divide texto en fragmentos inteligentes (~500 palabras)
- Preserva estructura sem√°ntica de p√°rrafos
- Genera metadatos detallados por fragmento
- Crea archivo JSON con todos los fragmentos y metadatos

REQUISITOS:
- Documentos Word en carpeta data/libros_word/
- Configuraci√≥n de libros en el script
- python-docx instalado

USO:
    python scripts/frag.py

FLUJO DE TRABAJO:
    1. Procesar documentos ‚Üí frag.py (este script)
    2. Generar embeddings ‚Üí embeddings.py
    3. Subir a Pinecone ‚Üí pinecone_u.py
    4. Usar chatbot ‚Üí consulta.py

CONFIGURACI√ìN DE LIBROS:
    libros_config = [
        {
            "archivo": "data/libros_word/G_A_S_4_E.docx",
            "nombre": "G_A_S_4_E"
        },
        {
            "archivo": "data/libros_word/LIBRO_IFSSA.docx", 
            "nombre": "LIBRO_IFSSA"
        }
    ]

EJEMPLO DE SALIDA:
    üöÄ Iniciando procesamiento de m√∫ltiples libros...
    üìñ Cargando documento: G_A_S_4_E
    ‚úÖ Texto extra√≠do de G_A_S_4_E: 75000 palabras
    ‚úÇÔ∏è Creando fragmentos...
    ‚úÖ Se crearon 150 fragmentos para G_A_S_4_E
    üéâ Procesamiento completado exitosamente
    üìä Estad√≠sticas generales:
       - Total de fragmentos: 300
       - Total de palabras: 150000
    üìö Estad√≠sticas por libro:
       - G_A_S_4_E: 150 fragmentos, 75000 palabras
       - LIBRO_IFSSA: 150 fragmentos, 75000 palabras

DEPENDENCIAS:
- python-docx: Para procesar documentos Word
- re: Para limpieza de texto
- json: Para guardar metadatos

AUTOR: Equipo de desarrollo ANA-Chatbot
FECHA: 2024
"""

import os
from docx import Document
import re
import json

def limpiar_texto(texto):
    """
    Limpia el texto eliminando caracteres especiales y normalizando espacios.
    
    Args:
        texto (str): Texto original a limpiar
        
    Returns:
        str: Texto limpio y normalizado
        
    Example:
        >>> texto_sucio = "Hola   mundo!!!   "
        >>> texto_limpio = limpiar_texto(texto_sucio)
        >>> print(texto_limpio)  # "Hola mundo"
    """
    texto = re.sub(r'[^\w\s\.\,\;\:\!\?\-\(\)√°√©√≠√≥√∫√º√±√Å√â√ç√ì√ö√ú√ë]', ' ', texto)
    texto = re.sub(r'\s+', ' ', texto)
    return texto.strip()

def dividir_texto_inteligente(texto, max_palabras=500):
    """
    Divide el texto en fragmentos inteligentes preservando la estructura sem√°ntica.
    
    Args:
        texto (str): Texto completo a dividir
        max_palabras (int): N√∫mero m√°ximo de palabras por fragmento. Default: 500
        
    Returns:
        List[str]: Lista de fragmentos de texto
        
    Example:
        >>> texto_largo = "P√°rrafo 1... P√°rrafo 2... P√°rrafo 3..."
        >>> fragmentos = dividir_texto_inteligente(texto_largo, max_palabras=300)
        >>> print(f"Se crearon {len(fragmentos)} fragmentos")
    """
    texto = limpiar_texto(texto)
    parrafos = [p.strip() for p in texto.split('\n') if p.strip()]
    
    fragmentos = []
    fragmento_actual = []
    palabras_actuales = 0
    
    for parrafo in parrafos:
        palabras_parrafo = parrafo.split()
        
        if len(palabras_parrafo) > max_palabras:
            oraciones = re.split(r'(?<=[.!?])\s+', parrafo)
            for oracion in oraciones:
                palabras_oracion = oracion.split()
                
                if palabras_actuales + len(palabras_oracion) <= max_palabras:
                    fragmento_actual.append(oracion)
                    palabras_actuales += len(palabras_oracion)
                else:
                    if fragmento_actual:
                        fragmentos.append(' '.join(fragmento_actual))
                    fragmento_actual = [oracion]
                    palabras_actuales = len(palabras_oracion)
        else:
            if palabras_actuales + len(palabras_parrafo) <= max_palabras:
                fragmento_actual.append(parrafo)
                palabras_actuales += len(palabras_parrafo)
            else:
                if fragmento_actual:
                    fragmentos.append(' '.join(fragmento_actual))
                fragmento_actual = [parrafo]
                palabras_actuales = len(palabras_parrafo)
    
    if fragmento_actual and palabras_actuales > 30:
        fragmentos.append(' '.join(fragmento_actual))
    
    return fragmentos

def procesar_libro(archivo_docx, nombre_libro, max_palabras=500):
    """
    Procesa un libro espec√≠fico y retorna sus fragmentos con metadatos.
    
    Args:
        archivo_docx (str): Ruta al archivo Word (.docx)
        nombre_libro (str): Nombre identificador del libro
        max_palabras (int): N√∫mero m√°ximo de palabras por fragmento. Default: 500
        
    Returns:
        List[Dict]: Lista de fragmentos con metadatos
        
    Example:
        >>> fragmentos = procesar_libro("data/libros_word/anatomia.docx", "ANATOMIA")
        >>> print(f"Procesados {len(fragmentos)} fragmentos")
    """
    if not os.path.exists(archivo_docx):
        print(f"‚ùå Error: No se encontr√≥ el archivo {archivo_docx}")
        return []
    
    try:
        print(f"üìñ Cargando documento: {nombre_libro}")
        doc = Document(archivo_docx)
        texto_completo = [p.text for p in doc.paragraphs if p.text.strip()]
        texto = '\n'.join(texto_completo)
        
        if not texto.strip():
            print(f"‚ùå Error: No se pudo extraer texto del documento {nombre_libro}")
            return []
        
        print(f"‚úÖ Texto extra√≠do de {nombre_libro}: {len(texto.split())} palabras")
        
        print("‚úÇÔ∏è Creando fragmentos...")
        fragmentos_texto = dividir_texto_inteligente(texto, max_palabras)
        
        # Crear fragmentos con metadatos
        fragmentos_con_metadata = []
        for idx, frag in enumerate(fragmentos_texto):
            palabras_frag = len(frag.split())
            fragmento_id = f"{nombre_libro}_fragmento_{idx+1:04d}"
            
            fragmento_data = {
                "id": fragmento_id,
                "texto": frag,
                "libro": nombre_libro,
                "palabras": palabras_frag,
                "indice": idx + 1
            }
            fragmentos_con_metadata.append(fragmento_data)
        
        print(f"‚úÖ Se crearon {len(fragmentos_con_metadata)} fragmentos para {nombre_libro}")
        return fragmentos_con_metadata
        
    except Exception as e:
        print(f"‚ùå Error al procesar {nombre_libro}: {str(e)}")
        return []

def main():
    """
    Funci√≥n principal que procesa m√∫ltiples libros de anatom√≠a.
    
    Esta funci√≥n:
    1. Define la configuraci√≥n de libros a procesar
    2. Procesa cada libro individualmente
    3. Combina todos los fragmentos
    4. Genera archivo de metadatos JSON con texto completo
    5. Proporciona estad√≠sticas detalladas
    
    Para agregar nuevos libros, modifica la lista libros_config.
    """
    # Obtener la ruta del directorio ra√≠z del proyecto
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(script_dir)
    
    # Configuraci√≥n de libros
    libros_config = [
        {
            "archivo": os.path.join(project_root, "data", "libros_word", "G_A_S_4_E.docx"),
            "nombre": "G_A_S_4_E"
        }
    ]
    
    max_palabras = 500
    todos_los_fragmentos = []
    
    print("üöÄ Iniciando procesamiento de m√∫ltiples libros...")
    
    # Procesar cada libro
    for libro_config in libros_config:
        archivo = libro_config["archivo"]
        nombre = libro_config["nombre"]
        
        if os.path.exists(archivo):
            fragmentos = procesar_libro(archivo, nombre, max_palabras)
            todos_los_fragmentos.extend(fragmentos)
        else:
            print(f"‚ö†Ô∏è Archivo no encontrado: {archivo}")
    
    if not todos_los_fragmentos:
        print("‚ùå No se pudieron procesar fragmentos de ning√∫n libro")
        return
    
    # Guardar metadatos completos con texto incluido
    metadata_file = os.path.join(project_root, "data", "fragmentos_metadata.json")
    with open(metadata_file, "w", encoding="utf-8") as f:
        json.dump(todos_los_fragmentos, f, ensure_ascii=False, indent=2)
    
    # Estad√≠sticas por libro
    libros_stats = {}
    for fragmento in todos_los_fragmentos:
        libro = fragmento["libro"]
        if libro not in libros_stats:
            libros_stats[libro] = {"fragmentos": 0, "palabras": 0}
        libros_stats[libro]["fragmentos"] += 1
        libros_stats[libro]["palabras"] += fragmento["palabras"]
    
    print(f"\nüéâ Procesamiento completado exitosamente")
    print(f"üìä Estad√≠sticas generales:")
    print(f"   - Total de fragmentos: {len(todos_los_fragmentos)}")
    print(f"   - Total de palabras: {sum(f['palabras'] for f in todos_los_fragmentos)}")
    
    print(f"\nüìö Estad√≠sticas por libro:")
    for libro, stats in libros_stats.items():
        print(f"   - {libro}: {stats['fragmentos']} fragmentos, {stats['palabras']} palabras")
    
    print(f"\nüíæ Archivo generado:")
    print(f"   - Metadatos completos con texto en '{metadata_file}'")

if __name__ == "__main__":
    main()
